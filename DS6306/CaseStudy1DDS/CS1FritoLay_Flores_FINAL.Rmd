---
title: "DDS 6306 Case Study 1 - Frito Lay Attrition"
author: "Michael Flores"
date: "2025-02-22"
output: html_document
---

```{r}

### Case Study 1 - Frito Lay Attrition Data -
## Description: DDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 100 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics has been hired by Frito Lay to identify factors related to employee attrition.  They aim to predict employee attrition in order to identify those employees who may be more likely to leave the company as well as identifying factors that are related to attrition so that they can best reduce the probability of attrition where desired. 

# The company has also indicated that they would like for you to build a model(s) to predict attrition and be able to measure the cost or savings impact of your model(s).  Specifically, they have identified, based on this article and prior research that it costs between 50% and 400% of the employee’s salary to recruit a replacement for someone who has left the company.  Additionally, they have estimated that if they give extra attention or incentives to employees that are about to leave they may be able to keep them from leaving.  Frito Lay has estimated the cost of these incentives to be $200 per employee.  These numbers can be used to help you estimate how much money a particular model could help save the company.  

# Frito Lay has provided a dataset (CaseStudy1-data.csv) to build your models and to do a data analysis to identify factors that lead to attrition.  You should identify the top three factors that contribute to turnover (backed up by evidence provided by analysis). Note: your model(s) may have more than three variables / features, we just want to specifically identify the top three most important. You may be able to / find it useful to create derived attributes/variables/features. 

# The business is also interested in learning about any other interesting trends and/or observations from your analysis. The analysis should be backed up by robust experimentation and appropriate visualization. Experiments and analysis must be conducted in R. You will also be asked to build a model to predict attrition.

# Executive Summary presentation can be seen by copying this link and pasting in a web browser: https://youtu.be/YFM6KdVgE5s

#load libraries
library(tidyr)
library(plyr)
library(jsonlite)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(plotly)
library(GGally)
library(class)
library(caret)
library(e1071)
library(corrplot)
library(ROSE)
library(pROC)

############## KNN Model
##############

# Load .csv data file
bandit = read.csv("C:/Users/Mike Flores/OneDrive/Documents/000SMU/6306_Doing Data Science/CSProject01/CaseStudy1-data.csv",header = TRUE)

# List size and structure of dataframe to see class and type
dim(bandit)
str(bandit)
head(bandit)

# Print top 5 rows to .csv file
write.csv(head(bandit), "bandit_head.csv", row.names = FALSE)

# Check columns Over18 and StandardHours to see what is in the values.
bandit %>% count(Over18)
bandit %>% count(StandardHours)

# Delete unnecessary columns; ID, EmployeeCount, EmployeeNumber, Over18 (only Yes), StandardHours (Only 80)
bandit = bandit[, -c(1,10,11,23,28)]

# Check for missing data
colSums(is.na(bandit))

# Check the size of the dataframe
dim(bandit)

# Check data types to see which need to be converted for preparation for KNN. If "numeric", will leave as is. All the categorical variables will be converted to a "factor".
str(bandit)

# Convert "target" response variable to factor with Yes/No classification.
bandit$Attrition = as.factor(bandit$Attrition)
# Check the levels for Attrition
levels (bandit$Attrition)
# Check the frequency of Attrition to validate which quantity is No and which is Yes
table(bandit$Attrition)
# Plot the Attrition frequencies
ggplot(bandit, aes(x = Attrition, fill = Attrition)) + geom_bar() + geom_text(aes(label = paste0(..count.., " (", round(..count.. / sum(..count..) * 100, 1), "%)")), stat = "count", vjust = -0.5, size = 4) + theme_minimal() + labs(title = "Attrition Frequency", x = "Attrition", y = "Count")

# Convert all all character type variables to factor type variables.
bandit$BusinessTravel = as.factor(bandit$BusinessTravel)
bandit$Department = as.factor(bandit$Department)
bandit$EducationField = as.factor(bandit$EducationField)
bandit$Gender = as.factor(bandit$Gender)
bandit$JobRole = as.factor(bandit$JobRole)
bandit$MaritalStatus = as.factor(bandit$MaritalStatus)
bandit$OverTime = as.factor(bandit$OverTime)
str(bandit)

# Check the statistical summary for all the quantitative variables and print to .csv file
summary_quan = names(bandit[sapply(bandit, is.numeric)])
summary(bandit[summary_quan])
write.csv(summary_quan, "summary_quan.csv", row.names = TRUE)

# Check the statistical summary for all the categorical variables and print to .csv file
summary_cat = names(bandit)[sapply(bandit, is.factor)]
summary(bandit[summary_cat])

# Create 5 number summarie boxplots and determine quantitative variables that have outliers
# Identify only numeric columns in the dataframe
numeric = names(bandit)[sapply(bandit, is.numeric)]

# Convert the numeric variables to long format, select only the numeric variables and reshape the data
numeric2 = bandit %>% select(all_of(numeric)) %>%  pivot_longer(cols = everything(), names_to = "NumericVar", values_to = "Value")

# Create facet-wrapped boxplots for all the quantitative variables to see 5 number summaries & outliers
ggplot(numeric2, aes(x = "", y = Value)) +  geom_boxplot(fill = "lightblue", outlier.color = "red", outlier.shape = 16) + facet_wrap(~NumericVar, scales = "free_y") + theme_minimal() + stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "green") + labs(x = "", y = "Value")

# Create facet-wrapped histograms for all the quantitative variables to see their frequency distribution with density curve overlay
ggplot(numeric2, aes(x = Value)) +  geom_histogram(bins = 30, fill = "skyblue", color = "black", alpha = 0.6) + facet_wrap(~NumericVar, scales = "free") + theme_minimal() + labs(x = "Value", y = "Frequency")

# Determine correlation between all quantitative variables and "Attrition".  So will convert Attrition to a numeric variable and then create a correlation matrix to evaluate the correlation between the variables.  Then we will determine which variables have the greatest influence on Attrition and which variable have negligible correlation and can be removed for the analysis.
# Create new dataframe for the correlation analysis
bandit_cor = bandit
# Convert "Attrition" to a numeric variable
bandit_cor$Attrition = as.numeric(as.factor(bandit_cor$Attrition)) - 1
unique(bandit_cor$Attrition)
# Identify only numeric columns in the dataframe
numeric = names(bandit_cor)[sapply(bandit_cor, is.numeric)]
# Compute the correlation matrix
numeric_cor = bandit_cor %>% select(all_of(numeric))  # Ensure numeric3 is a dataframe for cor() to function properly
summary(numeric_cor)
cormatrix = cor(numeric_cor, use = "complete.obs")
# Plot the correlation matrix
corrplot(cormatrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
# Export the correlation matrix to .csv file
print(cormatrix)
write.csv(cormatrix, "correlation_matrix.csv", row.names = TRUE)

# Convert "Attrition" to a numeric variable
bandit$Attrition = as.numeric(as.factor(bandit$Attrition)) - 1
unique(bandit$Attrition)
# Identify only numeric columns in the dataframe
numeric = names(bandit)[sapply(bandit, is.numeric)]

# Drop the quantitative variables that have low to no (between -0.1 & 0.1) correlation to "Attrition"
bandit_final = bandit[, -c(4,6,7,9,11,18,19,21,22,23,26,27,30)]
# Reorder the first two columns, Attrition first, then Age, then everything else.
bandit_final = bandit_final %>% select(2, 1, everything())
# Print new column order
print(names(bandit_final))
head(bandit_final)
# Print top 5 rows to .csv file
write.csv(head(bandit_final), "bandit_final.csv", row.names = FALSE)

# Identify only numeric columns in the dataframe
numeric3 = names(bandit_final)[sapply(bandit_final, is.numeric)]

# Convert the numeric variables to long format, select only the numeric variables and reshape the data
bandit_long = bandit_final %>% select(all_of(numeric3)) %>%  pivot_longer(cols = everything(), names_to = "NumericVar", values_to = "Value")

# Create a new dataframe excluding the 'Attrition' variable
bandit_quan = bandit_long %>% filter(NumericVar != "Attrition")

# Create facet-wrapped histograms for all the quantitative variables.
ggplot(bandit_quan, aes(x = Value, fill = Attrition)) +  geom_histogram(bins = 30, fill = "green", color = "black", alpha = 0.6, position = "identity") + facet_wrap(~NumericVar, scales = "free") + theme_minimal() + labs(x = "Value", y = "Frequency")+ labs(x = "Value", y = "Count")

ggplot(bandit_quan, aes(x = "", y = Value)) +  geom_boxplot(fill = "pink", outlier.color = "red", outlier.shape = 16) + facet_wrap(~NumericVar, scales = "free_y") + theme_minimal() + stat_summary(fun = mean, geom = "point", shape = 23, size = 3, fill = "blue") + labs(x = "", y = "Value")

# Visualize the categorical variables
# Identify only categorical columns in the dataframe
cat_vars = c("Attrition", "BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "OverTime")

# Create Attrition to factor for plotting
bandit_final$Attrition = as.factor(bandit_final$Attrition)

# Convert the categorical variables to long format, select only the categorical variables and reshape the data
cat = bandit_final %>% select(all_of(cat_vars)) %>%  pivot_longer(cols = -Attrition, names_to = "CatVar", values_to = "Category")
cat2 = bandit_final %>% select(all_of(cat_vars)) %>%  pivot_longer(cols = -Attrition, names_to = "CatVar", values_to = "Category")

# Compute percentages for Attrition
cat2_percent = cat %>% group_by(CatVar, Category) %>% summarise(count = n(), .groups = 'drop')%>%
  group_by(CatVar) %>% mutate(percent = (count / sum(count)) * 100)
cat3_percent = cat2 %>% group_by(CatVar, Category, Attrition) %>% summarise(count = n(), .groups = 'drop') %>% group_by(CatVar, Category) %>% mutate(percent = count / sum(count) * 100)

# Convert Attrition to a factor with "No" and "Yes" labels
bandit_final$Attrition = factor(bandit_final$Attrition)

# Create facet-wrapped barplots for all the categorical variables frequency
ggplot(cat2_percent, aes(x = Category, y = count, fill = Category)) +  geom_bar(stat = "identity") + geom_text(aes(label = paste0(round(percent, 1), "%")), vjust = -0.5, size = 3.5) + facet_wrap(~CatVar, scales = "free_x") + theme_minimal() + labs(x = "Category", y = "Count") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# Create facet-wrapped bar plots for all the categorical variables  vs Attrition variable.
ggplot(cat3_percent, aes(x = Category, y = count, fill = Attrition, group = Attrition)) + geom_bar(stat = "identity", position = "dodge") + geom_text(aes(label = paste0(round(percent, 1), "%")), position = position_dodge(width = 0.9), vjust = -0.3, size = 3) + facet_wrap(~CatVar, scales = "free_x") + theme_minimal() + labs(x = "Category", y = "Count", fill = "Attrition") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = "right") + guides(fill = guide_legend(title = "Attrition"))

# Create a new dataframe with only the three highest correlation variables vs Attrition.
bandit_3selected = bandit_final %>% select(Attrition, JobInvolvement, MaritalStatus, OverTime)

# Convert Attrition to a factor with "No" and "Yes" labels
bandit_3selected$Attrition = factor(bandit_3selected$Attrition)

# Visualize the three highest correlation variables vs Attrition.
cat_3var = c("Attrition", "OverTime", "JobInvolvement", "MaritalStatus")

# Create Attrition to factor for plotting
bandit_3selected$Attrition = as.factor(bandit_3selected$Attrition)

# Convert JobInvolvement to a factor
bandit_3selected$JobInvolvement = as.factor(bandit_3selected$JobInvolvement)

# Convert the categorical variables to long format, select only the categorical variables and reshape the data
cat3 = bandit_3selected %>% select(all_of(cat_3var)) %>%  pivot_longer(cols = -Attrition, names_to = "CatVar", values_to = "Category")

# Compute percentages for Attrition
cat3_percent = cat3 %>% group_by(CatVar, Category, Attrition) %>% summarise(count = n(), .groups = 'drop') %>% group_by(CatVar, Category) %>% mutate(percent = count / sum(count) * 100)

# Create facet-wrapped bar plots for the three highest correlation variables vs Attrition.
ggplot(cat3_percent, aes(x = Category, y = count, fill = Attrition, group = Attrition)) + 
  geom_bar(stat = "identity", position = "dodge") + 
  geom_text(aes(label = paste0(round(percent, 1), "%")), 
            position = position_dodge(width = 0.9), 
            vjust = -0.3, 
            size = 3) + 
  facet_wrap(~CatVar, scales = "free_x") + 
  theme_minimal() + 
  labs(x = "Category", y = "Count", fill = "Attrition") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), 
        legend.position = "right") + 
  guides(fill = guide_legend(title = "Attrition"))

# Convert JobInvolvement back to numeric
# bandit_3selected$JobInvolvement = as.numeric(bandit_3selected$JobInvolvement)

# KNN does not like factor variables, so will convert them to numeric before splitting the dataframe.
bandit_final$Attrition = as.numeric(as.factor(bandit_final$Attrition))
bandit_final$BusinessTravel = as.numeric(as.factor(bandit_final$BusinessTravel))
bandit_final$Department = as.numeric(as.factor(bandit_final$Department))
bandit_final$EducationField = as.numeric(as.factor(bandit_final$EducationField))
bandit_final$Gender = as.numeric(as.factor(bandit_final$Gender))
bandit_final$JobRole = as.numeric(as.factor(bandit_final$JobRole))
bandit_final$MaritalStatus = as.numeric(as.factor(bandit_final$MaritalStatus))
bandit_final$OverTime = as.numeric(as.factor(bandit_final$OverTime))
# Final dataframe for analysis
str(bandit_final)
 
# Determine final correlation between all final variables and "Attrition".
# Create a new dataframe for the correlation analysis. Copy to avoid modifying bandit_final
bandit_cor = bandit_final 
# Convert "Attrition" to numeric only for correlation analysis
bandit_cor$Attrition = as.numeric(as.factor(bandit_cor$Attrition)) - 1
# Identify only numeric columns in the dataframe
numeric_vars = names(bandit_cor)[sapply(bandit_cor, is.numeric)]
# Compute the correlation matrix. ensure it's a dataframe
numeric_cor = bandit_cor %>% select(all_of(numeric_vars))
cormatrix = cor(numeric_cor, use = "complete.obs")
# Plot the correlation matrix
corrplot(cormatrix, method = "color", type = "upper", tl.col = "black", tl.srt = 45)
# Export the correlation matrix to a .csv file
write.csv(cormatrix, "correlation_matrix_final.csv", row.names = TRUE)

# Standardize all numeric features except Attrition which is the target variable and a categorical variable.
# Create a new dataframe that is scaled for all the numeric variables without Attrition.
bandit_scaled = as.data.frame(scale(bandit_final[, -1]))
# Add the Attrition column back to the scaled dataframe.
bandit_scaled$Attrition = bandit_final$Attrition
head(bandit_scaled)

# Use a 70/30 train/test dataset split
set.seed(1)
splitPerc = .70
trainIndices = sample(1:dim(bandit_scaled)[1],round(splitPerc * dim(bandit_scaled)[1]))   # Create a 70-30 train-test split
train = bandit_scaled[trainIndices,]
test = bandit_scaled[-trainIndices,]

# Since cannot get Specificity to rise with changes to Threshold (.1 - .9 showed little impact), have recognized there is a class imbalance.  Therefore, will check class imbalance.
table(train$Attrition)

# Apply Oversampling to Balance the Data.  Using Oversampling and not Undersampling since the dataset is small.
# Double dataset size
train_balanced = ROSE(Attrition ~ ., data = train, seed = 1, N = nrow(train) * 2)$data

# Check new balance
table(train_balanced$Attrition)

# Loop for many k and the average of many training / test partition
# Tune the hyperparameter k
set.seed(1)
iterations = 500  # Number of repetitions
numks = 90       # Number of k values to test

masterAcc = matrix(nrow = iterations, ncol = numks)

for(j in 1:iterations)
{
  trainIndices = sample(1:dim(train_balanced)[1],round(splitPerc * dim(train_balanced)[1]))
  train_sampled = train_balanced[trainIndices,]

  for(i in 1:numks)
  {
    classifications = knn(train_sampled[,-ncol(train_sampled)],test[,-ncol(test)],train_sampled$Attrition, prob = TRUE, k = i)
    table(classifications,test$Attrition)
    CM = confusionMatrix(table(classifications,test$Attrition)) # Compute the confusion matrix and accuracy
    masterAcc[j,i] = CM$overall[1]
  }
}

# Compute mean accuracy for each k
MeanAcc = colMeans(masterAcc)

# Plot k vs accuracy
plot(seq(1,numks,1),MeanAcc, type = "l", xlab = "k (Number of Nearest Neighbors)", ylab = "Accuracy", main = "Tuning k in kNN")

# Print console output with best values
k_best = which.max(MeanAcc)   # Find the best k
acc_best = max(MeanAcc)   # Find the best accuracy
cat("Best k:", k_best, "Max Accuracy:", acc_best) 

# Use the best k value = 35 from loop but not 60% for Specificity, only achieves 59%.  Therefore will followup with manual adjustments to achieve 60%.
Attritions_pred = knn(train_balanced[,-ncol(train_balanced)], test[,-ncol(test)], train_balanced$Attrition, k = k_best)

# Compute and print Confusion Matrix
table(Attritions_pred, test$Attrition)
confusionMatrix(table(Attritions_pred, test$Attrition))

# Use the final k value = 45 to achieve 60% for Specificity (used manual increments from k = 35-50).  Raised k value 10 points to achieve 60% Specificity.
Attritions_pred = knn(train_balanced[,-ncol(train_balanced)], test[,-ncol(test)], train_balanced$Attrition, k = 45)

# Compute and print Confusion Matrix
table(Attritions_pred, test$Attrition)

# Convert Attritions_pred to a factor matching test$Attrition levels
Attritions_pred = factor(Attritions_pred, levels = c(1, 2), labels = c("No", "Yes"))

# Convert test$Attrition back to a factor with "No" and "Yes"
test$Attrition = factor(test$Attrition, levels = c(1, 2), labels = c("No", "Yes"))
CM_Attrition = confusionMatrix(table(Attritions_pred, test$Attrition), positive = "Yes")
# Print the updated confusion matrix
print(CM_Attrition)

# Cost analysis follows.
# Defined cost parameters.
CostAttrition = 10000       # Cost of an employee leaving, assumed value.
CostRetentionIntervention = 2500  # Cost of retention intervention, assumed value.

# Extract confusion matrix values
TN = CM_Attrition$table[1,1]  # True Negatives (No correctly predicted as No)
FN = CM_Attrition$table[1,2]  # False Negatives (Yes but predicted as No)
FP = CM_Attrition$table[2,1]  # False Positives (No but predicted as Yes)
TP = CM_Attrition$table[2,2]  # True Positives (Yes correctly predicted as Yes)

# Compute total cost
TotalCost = (FN * CostAttrition) + (FP * CostRetentionIntervention)

# Print total cost
print(paste("Total Cost of Attrition Management:", TotalCost))


##############KNN Model Prediction for Holdout Set with .csv file
##############

# Load validation dataset
validation = read.csv("C:/Users/Mike Flores/OneDrive/Documents/000SMU/6306_Doing Data Science/CSProject01/CaseStudy1CompSet No Attrition.csv", header = TRUE)

str(validation)

# Store Employee ID separately before removing unnecessary columns since its been deleted from training dataset.
validation_IDs = validation$ID  # Keep Employee ID for final output

# Remove unnecessary columns (same as training dataset)
validation = validation[, -c(1,4,6,7,9,10,11,13,20,21,22,24,25,26,27,30,31,34)]

# Check for missing data
colSums(is.na(validation))

# Check the size of the dataframe
dim(validation)

# Check data types to see which need to be converted for preparation for KNN. If "numeric", will leave as is. All the categorical variables will be converted to a "factor".
str(validation)

# Create OverTime column in validation dataframe to match training dataset since this column is missing from CaseStudy1CompSet No Attrition.csv file.  Assume there is no OverTime in the dataset so populate each row with "No".
validation$OverTime = "No"
# Ensure all values are set to "No"
validation$OverTime[is.na(validation$OverTime)] = "No"

# Convert categorical columns to factors
validation$BusinessTravel = as.factor(validation$BusinessTravel)
validation$Department = as.factor(validation$Department)
validation$EducationField = as.factor(validation$EducationField)
validation$Gender = as.factor(validation$Gender)
validation$JobRole = as.factor(validation$JobRole)
validation$MaritalStatus = as.factor(validation$MaritalStatus)
validation$OverTime = as.factor(validation$OverTime)

# Convert categorical variables to numeric (same as training dataset)
validation$BusinessTravel = as.numeric(as.factor(validation$BusinessTravel))
validation$Department = as.numeric(as.factor(validation$Department))
validation$EducationField = as.numeric(as.factor(validation$EducationField))
validation$Gender = as.numeric(as.factor(validation$Gender))
validation$JobRole = as.numeric(as.factor(validation$JobRole))
validation$MaritalStatus = as.numeric(as.factor(validation$MaritalStatus))
validation$OverTime = as.numeric(as.factor(validation$OverTime))

# Standardize the validation dataset (must match training dataset)
validation_scaled = as.data.frame(scale(validation))
# Check for missing values in validation_scaled dataframe
colSums(is.na(validation_scaled))
# Replace missing OverTime in validation_scaled dataframe with 0 = no overtime.
validation_scaled$OverTime[is.na(validation_scaled$OverTime)] = 0
colSums(is.na(validation_scaled))

# Predict Attrition for the validation dataset using the trained KNN model
Attritions_pred_valid = knn(train_balanced[,-ncol(train_balanced)], validation_scaled, train_balanced$Attrition, k = k_best)

# Convert predictions to factor labels (Yes/No)
Attritions_pred_valid = factor(Attritions_pred_valid, levels = c(1, 2), labels = c("No", "Yes"))

# Create a dataframe with Employee ID and Predicted Attrition
final_predictions = data.frame(ID = validation_IDs, Attrition = Attritions_pred_valid)

# Save to a CSV file
write.csv(final_predictions, "Case1PredictionsFlores Attrition.csv", row.names = FALSE)


##############Naive Bayer Model
##############

# Load .csv data file
bandit = read.csv("C:/Users/Mike Flores/OneDrive/Documents/000SMU/6306_Doing Data Science/CSProject01/CaseStudy1-data.csv",header = TRUE)

# Delete unnecessary columns; ID, EmployeeCount, EmployeeNumber, Over18 (only Yes), StandardHours (Only 80)
bandit = bandit[, -c(1,10,11,23,28)]

# Convert "target" response variable to factor with Yes/No classification.
bandit$Attrition = as.factor(bandit$Attrition)
# Convert all all character type variables to factor type variables.
bandit$BusinessTravel = as.factor(bandit$BusinessTravel)
bandit$Department = as.factor(bandit$Department)
bandit$EducationField = as.factor(bandit$EducationField)
bandit$Gender = as.factor(bandit$Gender)
bandit$JobRole = as.factor(bandit$JobRole)
bandit$MaritalStatus = as.factor(bandit$MaritalStatus)
bandit$OverTime = as.factor(bandit$OverTime)
str(bandit)

# Drop the quantitative variables that have low to no (between -0.1 & 0.1) correlation to "Attrition"
bandit_final = bandit[, -c(4,6,7,9,11,18,19,21,22,23,26,27,30)]

# Final dataframe for analysis
str(bandit_final)
# Reorder the first two columns
bandit_final = bandit_final %>% select(2, 1, everything())
# Print new column order
print(names(bandit_final))
head(bandit_final)

# Shuffle the dataframe bandit_final
set.seed(6)
bandit_shuffle = runif(nrow(bandit_final))
bandit_s = bandit_final[order(bandit_shuffle), ]

# Split the data into 70/30, train (rows 1-609) and test (rows 610-870) 
train = bandit_s[1:609, ]
test = bandit_s[610:870, ]

# Train a Naïve Bayes model using only train dataset and all the variables as predictors
model = naiveBayes(Attrition~ ., data = train)

# Make predictions on the test dataset
predictions = predict(model, test)

# Evaluate Naive Bayer model performance with the CM
table(Predicted = predictions, Actual = test$Attrition)
confusionMatrix(predictions, test$Attrition)

# Define a new threshold to identify "Attritions" more liberally
threshold = .3

probsNB = predict(model, test, type = "raw")

# Apply the new threshold to reclassify observations
Attrition = ifelse(probsNB[, "Yes"] >= threshold, "Yes", "No")
# Convert to factor to match test set levels
Attrition = factor(Attrition, levels = levels(test$Attrition))
# Tabulate the new classifications against actual values
table(Attrition, test$Attrition)

# Compute a confusion matrix to evaluate the accuracy of predictions; using all available measures
CM_NB = confusionMatrix(table(Attrition, test$Attrition), mode = "everything")
# Display the confusion matrix for NB
CM_NB

# Confusion matrix for KNN is shows better performance, therefore will use KNN model for predictions and cost analysis.  See cost analysis under KNN section above




```

